{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load persuasion data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            source                                              claim  \\\n",
      "236  Claude 3 Opus  Governments and technology companies must do m...   \n",
      "349  Claude 3 Opus  Governments and technology companies must do m...   \n",
      "430  Claude 3 Opus  Governments and technology companies must do m...   \n",
      "518  Claude 3 Opus  Governments and technology companies must do m...   \n",
      "\n",
      "                 prompt_type  \\\n",
      "236                Deceptive   \n",
      "349        Logical Reasoning   \n",
      "430  Expert Writer Rhetorics   \n",
      "518          Compelling Case   \n",
      "\n",
      "                                              argument  \n",
      "236  Online privacy and security are under daily as...  \n",
      "349  In the modern digital age, most of us share va...  \n",
      "430  In today's interconnected digital world, our p...  \n",
      "518  In the digital age, protecting online privacy ...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/my_honours/persuasion_data.csv\")\n",
    "data = data.loc[data[\"source\"] != \"Control\"]\n",
    "# Let's just do a massively shorter dataset for now\n",
    "# Only Claude 3 Opus and a single claim \n",
    "data = data.loc[\n",
    "    (data[\"source\"] == \"Claude 3 Opus\") &\n",
    "    (data[\"claim\"] == data[\"claim\"][0])\n",
    "]\n",
    "print(data[[\"source\", \"claim\", \"prompt_type\", \"argument\"]].drop_duplicates())\n",
    "\n",
    "processed_df = {}\n",
    "for entry in data.itertuples():\n",
    "    argument = entry.argument\n",
    "    if argument not in processed_df:\n",
    "        processed_df[argument] = {\n",
    "            'input': f\"Write a persuasive argument for the claim: {entry.claim}\",\n",
    "            'output': entry.argument,\n",
    "            'source': entry.source,\n",
    "            'prompt_type': entry.prompt_type,\n",
    "            'rating': [entry.rating_final_number],\n",
    "        }\n",
    "    else:\n",
    "        processed_df[argument]['rating'].append(entry.rating_final_number)\n",
    "\n",
    "input = [dp['input'] for dp in list(processed_df.values())]\n",
    "output = [dp['output'] for dp in list(processed_df.values())]\n",
    "scores = [round(np.mean(dp['rating']),1) for dp in list(processed_df.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_openai_api_key():\n",
    "    os.environ.pop(\"OPENAI_API_KEY\", None)\n",
    "    load_dotenv()\n",
    "\n",
    "reset_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of summary candidates: 4\n"
     ]
    }
   ],
   "source": [
    "from utils import shuffle_lists, calculate_correlation, load_newsroom, load_summEval, calculate_uncertainty, load_sf_data, CompareResultObject, insert_index_to_anchors\n",
    "\n",
    "print('Number of summary candidates:', len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set the meta-parameters\n",
    "params = {\n",
    "    'dataset': 'SummEval',\n",
    "    'engine': \"gpt-3.5-turbo\",\n",
    "    'aspect': 'coherence',\n",
    "    'eval_method': 'pairwise comparison',\n",
    "    'confidence_beam': False,  # False for PairS-greedy search\n",
    "    # 'beam_size': 2000,\n",
    "    # 'prob_gap': 0.1,\n",
    "    'api_call': 0,\n",
    "    'with_input': True,\n",
    "    'compare_log': {},\n",
    "    'calibration': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitchellmoore/Library/Caches/pypoetry/virtualenvs/pairs-PZQRVcco-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Processing:   0%|          | 0/8 [00:00<?, ?it/s]/Users/mitchellmoore/code/research/honours/persuasion_project/PairS/pairs/utils.py:44: RuntimeWarning: invalid value encountered in log\n",
      "  entropy = -np.sum(probablities * np.log(probablities))\n",
      "Processing:  62%|██████▎   | 5/8 [00:03<00:02,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from sorting import merge_sort_indices, merge_sort\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Set the progress bar\n",
    "if params['confidence_beam']:\n",
    "    params['progress_bar'] = tqdm(total=int(len(input)**2), desc='Processing')\n",
    "else:\n",
    "    params['progress_bar'] = tqdm(total=int(len(input) * np.log2(len(input))), desc='Processing')\n",
    "\n",
    "# Shuffle the input, output, and scores\n",
    "input, output, scores = shuffle_lists(input, output, scores)\n",
    "\n",
    "# Perform the PairS-greedy ranking\n",
    "# Please note: All prompts are saved in /scripts/prompts.py\n",
    "ranking_indices = merge_sort_indices(input, output, params)\n",
    "\n",
    "params['progress_bar'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.316227766016838, 0.18257418583505539)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the correlation\n",
    "spearman_corr, kendall_tau = calculate_correlation(np.array(scores)[ranking_indices], list(range(len(scores))))\n",
    "\n",
    "spearman_corr, kendall_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "236                Deceptive   \n",
    "349        Logical Reasoning   \n",
    "430  Expert Writer Rhetorics   \n",
    "518          Compelling Case \n",
    "\n",
    "Logical\n",
    "Deceptive\n",
    "Expert Writer\n",
    "Compelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/SummEval/model_annotations.aligned.paired.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load example data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m summ_eval_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/SummEval/model_annotations.aligned.paired.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m input_doc, output_doc, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_summEval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msumm_eval_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m input_doc[doc_id], output_doc[doc_id]\n",
      "File \u001b[0;32m~/code/research/honours/persuasion_project/PairS/scripts/utils.py:135\u001b[0m, in \u001b[0;36mload_summEval\u001b[0;34m(path, flat_output)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_summEval\u001b[39m(path, flat_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 135\u001b[0m     data_summ_eval \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_summ_eval)):\n",
      "File \u001b[0;32m~/code/research/honours/persuasion_project/PairS/scripts/utils.py:124\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_jsonl\u001b[39m(file_path):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    125\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/SummEval/model_annotations.aligned.paired.jsonl'"
     ]
    }
   ],
   "source": [
    "from pairs import PairsGreedy, PairsBeam\n",
    "from scripts.utils import shuffle_lists, load_summEval\n",
    "\n",
    "\n",
    "# Load example data\n",
    "summ_eval_path = 'data/SummEval/model_annotations.aligned.paired.jsonl'\n",
    "input_doc, output_doc, _ = load_summEval(summ_eval_path, flat_output=False)\n",
    "\n",
    "doc_id = 42\n",
    "input, output = input_doc[doc_id], output_doc[doc_id]\n",
    "input, output = shuffle_lists(input, output)\n",
    "\n",
    "# The same input source text corresponds to multiple output summaries\n",
    "print('Number of summary candidates:', len(output))\n",
    "\n",
    "method = 'PairsGreedy'\n",
    "if method == 'PairsGreedy':\n",
    "    # Set hyperparameters\n",
    "    params = {\n",
    "        # 'engine': \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        'engine': \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        'api_call': 0,\n",
    "        'with_input': True,   # Use the prompt template for task with context input, e.g. Summarization \n",
    "        'calibrate': False,   # For each pairwise comparison, we average the probabilities of both permutations to cancel the positional bias.\n",
    "    }\n",
    "    # Rank the output summaries from low to high quality\n",
    "    indices = PairsGreedy(input[0], output, params)\n",
    "    print(indices)\n",
    "\n",
    "elif method == 'PairsBeam':\n",
    "    # Set hyperparameters\n",
    "    params = {\n",
    "        'engine': \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        'beam_size': 2000,\n",
    "        'api_call': 0,\n",
    "        'prob_gap': 0.1,\n",
    "        'with_input': True,\n",
    "        'calibrate': False,\n",
    "    }\n",
    "    # Rank the output summaries from low to high quality\n",
    "    indices = PairsBeam(input[0], output, params)\n",
    "    print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairs-PZQRVcco-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
